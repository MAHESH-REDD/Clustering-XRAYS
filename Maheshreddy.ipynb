{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09de57f8-c116-4e1c-a97d-db28265447ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11712 images in C:\\Users\\BHARGAV NAIDU\\Downloads\\archive (2)\\chest_xray\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|█████████████████████████████████████████████████████████| 11712/11712 [09:58<00:00, 19.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 11712 feature vectors. Failed: 0\n",
      "Feature matrix shape: (11712, 34918)\n",
      "Elapsed (feature extraction): 600.7s\n",
      "Saved scaled features at clustering_outputs\\xray_features_scaled.csv\n",
      "Running PCA to reduce to 100 components...\n",
      "Explained variance (sum): 0.3255538243200587\n",
      "Running KMeans for K in [2, 3, 4, 5, 6]\n",
      "Best K (by silhouette) = 2\n",
      "Saved clustering evaluation to clustering_outputs\\clustering_evaluation.csv\n",
      "          method  n_clusters  silhouette  davies_bouldin\n",
      "0         kmeans           2    0.083631        3.214753\n",
      "1  agglomerative           2    0.075555        3.462432\n",
      "2         dbscan           2    0.031891        0.861130\n",
      "Running PCA (for t-SNE) to 50 components then t-SNE (2D)...\n",
      "Saved clusters CSV to clustering_outputs\\xray_features_with_clusters.csv\n",
      "Saved cluster purity summary to clustering_outputs\\cluster_purity.csv\n",
      "   method    purity\n",
      "0  kmeans  0.729679\n",
      "1     agg  0.729679\n",
      "2  dbscan  0.729679\n",
      "All outputs saved to directory: C:\\Users\\BHARGAV NAIDU\\clustering_outputs\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from skimage.feature import hog, local_binary_pattern\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "DATA_DIR = os.path.expanduser(r\"chest_xray\") \n",
    "IMG_SIZE = (256, 256)\n",
    "HIST_BINS = 256\n",
    "HOG_PIXELS_PER_CELL = (8, 8)\n",
    "HOG_CELLS_PER_BLOCK = (2, 2)\n",
    "HOG_ORIENTATIONS = 9\n",
    "LBP_P = 8\n",
    "LBP_R = 1\n",
    "LBP_N_BINS = 59\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "PCA_N_COMPONENTS_FOR_CLUSTERING = 100   # reduce to this many principal components for clustering\n",
    "PCA_N_COMPONENTS_FOR_TSNE = 50          # PCA before t-SNE (speeds up t-SNE)\n",
    "TSNE_PERPLEXITY = 30\n",
    "\n",
    "# KMeans selection\n",
    "K_RANGE = range(2, 7)  # try K from 2..6\n",
    "KMEANS_N_INIT = 10\n",
    "\n",
    "# DBSCAN\n",
    "DBSCAN_EPS = 2.5\n",
    "DBSCAN_MINPTS = 5\n",
    "\n",
    "# OUTPUT FILES\n",
    "FEATURES_CSV = \"xray_features_scaled.csv\"\n",
    "CLUSTERS_CSV = \"xray_features_with_clusters.csv\"\n",
    "EVAL_CSV = \"clustering_evaluation.csv\"\n",
    "OUT_DIR = \"clustering_outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def find_images(root_dir, exts=('png','jpg','jpeg','bmp')):\n",
    "    files = []\n",
    "    for e in exts:\n",
    "        files.extend(glob(os.path.join(root_dir, '**', f'*.{e}'), recursive=True))\n",
    "    return sorted(files)\n",
    "\n",
    "def read_preprocess(path, size=IMG_SIZE):\n",
    "    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "    if img is None:\n",
    "        return None\n",
    "    if len(img.shape) == 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = cv2.resize(img, size, interpolation=cv2.INTER_AREA)\n",
    "    if img.dtype != np.uint8:\n",
    "        img = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "def hist_feat(img, bins=HIST_BINS):\n",
    "    hist = cv2.calcHist([img], [0], None, [bins], [0,256]).flatten()\n",
    "    hist = hist / (hist.sum() + 1e-8)\n",
    "    return hist\n",
    "\n",
    "def hog_feat(img):\n",
    "    img_f = img.astype('float32') / 255.0\n",
    "    v = hog(img_f,\n",
    "            orientations=HOG_ORIENTATIONS,\n",
    "            pixels_per_cell=HOG_PIXELS_PER_CELL,\n",
    "            cells_per_block=HOG_CELLS_PER_BLOCK,\n",
    "            block_norm='L2-Hys',\n",
    "            feature_vector=True)\n",
    "    return v\n",
    "\n",
    "def lbp_hist_feat(img, P=LBP_P, R=LBP_R, bins=LBP_N_BINS):\n",
    "    lbp = local_binary_pattern(img, P, R, method='uniform')\n",
    "    (hist, _) = np.histogram(lbp.ravel(), bins=np.arange(0, bins+1), range=(0, bins))\n",
    "    hist = hist.astype('float32')\n",
    "    hist = hist / (hist.sum() + 1e-8)\n",
    "    return hist\n",
    "\n",
    "def hu_moments_feat(img):\n",
    "    m = cv2.moments(img.astype('float32'))\n",
    "    hu = cv2.HuMoments(m).flatten()\n",
    "    with np.errstate(divide='ignore'):\n",
    "        hu_log = -np.sign(hu) * np.log10(np.abs(hu) + 1e-30)\n",
    "    hu_log = np.nan_to_num(hu_log)\n",
    "    return hu_log\n",
    "\n",
    "def extract_label(path):\n",
    "    parts = path.replace('\\\\','/').split('/')\n",
    "    for tok in reversed(parts[:-1]):\n",
    "        t = tok.upper()\n",
    "        if 'NORMAL' in t:\n",
    "            return 'NORMAL'\n",
    "        if 'PNEUMONIA' in t:\n",
    "            return 'PNEUMONIA'\n",
    "    return 'UNKNOWN'\n",
    "\n",
    "image_paths = find_images(DATA_DIR)\n",
    "print(f\"Found {len(image_paths)} images in {DATA_DIR}\")\n",
    "\n",
    "features = []\n",
    "paths = []\n",
    "true_labels = []\n",
    "failed = 0\n",
    "start = time.time()\n",
    "\n",
    "for p in tqdm(image_paths, desc=\"Processing images\"):\n",
    "    img = read_preprocess(p)\n",
    "    if img is None:\n",
    "        failed += 1\n",
    "        continue\n",
    "    v_hist = hist_feat(img)\n",
    "    v_hog = hog_feat(img)\n",
    "    v_lbp = lbp_hist_feat(img)\n",
    "    v_hu  = hu_moments_feat(img)\n",
    "    v = np.concatenate([v_hist, v_hog, v_lbp, v_hu])\n",
    "    features.append(v)\n",
    "    paths.append(p)\n",
    "    true_labels.append(extract_label(p))\n",
    "\n",
    "features = np.array(features)\n",
    "print(f\"Loaded {features.shape[0]} feature vectors. Failed: {failed}\")\n",
    "print(\"Feature matrix shape:\", features.shape)\n",
    "print(\"Elapsed (feature extraction): {:.1f}s\".format(time.time() - start))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(features)\n",
    "# Save scaled features with metadata\n",
    "df_feat = pd.DataFrame(X_scaled)\n",
    "df_feat['path'] = paths\n",
    "df_feat['true_label'] = true_labels\n",
    "df_feat.to_csv(os.path.join(OUT_DIR, FEATURES_CSV), index=False)\n",
    "print(\"Saved scaled features at\", os.path.join(OUT_DIR, FEATURES_CSV))\n",
    "\n",
    "print(\"Running PCA to reduce to\", PCA_N_COMPONENTS_FOR_CLUSTERING, \"components...\")\n",
    "pca_cluster = PCA(n_components=PCA_N_COMPONENTS_FOR_CLUSTERING, random_state=RANDOM_STATE)\n",
    "X_pca_cluster = pca_cluster.fit_transform(X_scaled)\n",
    "print(\"Explained variance (sum):\", pca_cluster.explained_variance_ratio_.sum())\n",
    "\n",
    "inertias = []\n",
    "sil_scores = []\n",
    "ks = list(K_RANGE)\n",
    "print(\"Running KMeans for K in\", ks)\n",
    "for k in ks:\n",
    "    km = KMeans(n_clusters=k, n_init=KMEANS_N_INIT, random_state=RANDOM_STATE)\n",
    "    lab = km.fit_predict(X_pca_cluster)\n",
    "    inertias.append(km.inertia_)\n",
    "    try:\n",
    "        sil = silhouette_score(X_pca_cluster, lab)\n",
    "    except:\n",
    "        sil = np.nan\n",
    "    sil_scores.append(sil)\n",
    "\n",
    "# Save elbow and silhouette plots\n",
    "plt.figure()\n",
    "plt.plot(ks, inertias, marker='o')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow: K vs Inertia')\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(OUT_DIR, 'k_elbow.png'), dpi=150)\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(ks, sil_scores, marker='o')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette: K vs Score')\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(OUT_DIR, 'k_silhouette.png'), dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# Choose K: pick K with max silhouette (fallback to 2)\n",
    "best_k_idx = int(np.nanargmax(sil_scores)) if any(~np.isnan(sil_scores)) else 0\n",
    "best_k = ks[best_k_idx] if ks else 2\n",
    "print(\"Best K (by silhouette) =\", best_k)\n",
    "\n",
    "# Run final KMeans with best_k\n",
    "kmeans = KMeans(n_clusters=best_k, n_init=KMEANS_N_INIT, random_state=RANDOM_STATE)\n",
    "labels_kmeans = kmeans.fit_predict(X_pca_cluster)\n",
    "\n",
    "# ----------------------\n",
    "# Agglomerative (on PCA space)\n",
    "# ----------------------\n",
    "agg = AgglomerativeClustering(n_clusters=best_k, linkage='ward')\n",
    "labels_agg = agg.fit_predict(X_pca_cluster)\n",
    "\n",
    "# ----------------------\n",
    "# DBSCAN (on PCA space)\n",
    "# ----------------------\n",
    "db = DBSCAN(eps=DBSCAN_EPS, min_samples=DBSCAN_MINPTS, metric='euclidean')\n",
    "labels_dbscan = db.fit_predict(X_pca_cluster)\n",
    "\n",
    "\n",
    "def safe_metrics(X, labels):\n",
    "    res = {'silhouette': None, 'davies_bouldin': None, 'n_clusters': None}\n",
    "    lab_unique = set(labels)\n",
    "    lab_non_noise = [l for l in lab_unique if l != -1]\n",
    "    res['n_clusters'] = len(lab_non_noise) if -1 in lab_unique else len(lab_unique)\n",
    "    try:\n",
    "        if len(set(labels)) > 1:\n",
    "            res['silhouette'] = float(silhouette_score(X, labels))\n",
    "            res['davies_bouldin'] = float(davies_bouldin_score(X, labels))\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    return res\n",
    "\n",
    "evals = []\n",
    "eval_km = safe_metrics(X_pca_cluster, labels_kmeans)\n",
    "eval_km.update({'method': 'kmeans', 'labels_array': labels_kmeans})\n",
    "evals.append(eval_km)\n",
    "eval_agg = safe_metrics(X_pca_cluster, labels_agg)\n",
    "eval_agg.update({'method': 'agglomerative', 'labels_array': labels_agg})\n",
    "evals.append(eval_agg)\n",
    "eval_db = safe_metrics(X_pca_cluster, labels_dbscan)\n",
    "eval_db.update({'method': 'dbscan', 'labels_array': labels_dbscan})\n",
    "evals.append(eval_db)\n",
    "\n",
    "eval_df = pd.DataFrame([{\n",
    "    'method': e['method'],\n",
    "    'n_clusters': e['n_clusters'],\n",
    "    'silhouette': e['silhouette'],\n",
    "    'davies_bouldin': e['davies_bouldin']\n",
    "} for e in evals])\n",
    "eval_df.to_csv(os.path.join(OUT_DIR, EVAL_CSV), index=False)\n",
    "print(\"Saved clustering evaluation to\", os.path.join(OUT_DIR, EVAL_CSV))\n",
    "print(eval_df)\n",
    "\n",
    "# ----------------------\n",
    "# t-SNE visualization (PCA -> t-SNE for speed)\n",
    "# ----------------------\n",
    "print(\"Running PCA (for t-SNE) to\", PCA_N_COMPONENTS_FOR_TSNE, \"components then t-SNE (2D)...\")\n",
    "pca_tsne = PCA(n_components=PCA_N_COMPONENTS_FOR_TSNE, random_state=RANDOM_STATE)\n",
    "X_pca_tsne = pca_tsne.fit_transform(X_scaled)\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=TSNE_PERPLEXITY, init='pca', random_state=RANDOM_STATE, learning_rate='auto')\n",
    "X_tsne = tsne.fit_transform(X_pca_tsne)\n",
    "\n",
    "# Plot function\n",
    "def plot_labels_2d(coords, labels, title, fname):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    uniq = np.unique(labels)\n",
    "    for lab in uniq:\n",
    "        mask = labels == lab\n",
    "        label_name = 'noise' if lab == -1 else str(lab)\n",
    "        plt.scatter(coords[mask,0], coords[mask,1], s=8, label=label_name, alpha=0.7)\n",
    "    plt.legend(markerscale=2, bbox_to_anchor=(1.05,1), loc='upper left')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUT_DIR, fname), dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "plot_labels_2d(X_tsne, labels_kmeans, f\"KMeans (K={best_k}) t-SNE\", \"kmeans_tsne.png\")\n",
    "plot_labels_2d(X_tsne, labels_agg, f\"Agglomerative (K={best_k}) t-SNE\", \"agg_tsne.png\")\n",
    "plot_labels_2d(X_tsne, labels_dbscan, f\"DBSCAN (eps={DBSCAN_EPS}, minpts={DBSCAN_MINPTS}) t-SNE\", \"dbscan_tsne.png\")\n",
    "\n",
    "# Plot true labels (if available)\n",
    "if len(set(true_labels)) > 1:\n",
    "    true_enc, uniques = pd.factorize(true_labels)\n",
    "    plot_labels_2d(X_tsne, true_enc, \"True labels (t-SNE)\", \"true_labels_tsne.png\")\n",
    "\n",
    "# ----------------------\n",
    "# Save full CSV with cluster labels\n",
    "# ----------------------\n",
    "df_out = pd.DataFrame({\n",
    "    'path': paths,\n",
    "    'true_label': true_labels,\n",
    "    'kmeans_label': labels_kmeans,\n",
    "    'agg_label': labels_agg,\n",
    "    'dbscan_label': labels_dbscan\n",
    "})\n",
    "# Append first few PCA components for convenience\n",
    "for i in range(min(10, X_pca_cluster.shape[1])):\n",
    "    df_out[f'pca_{i+1}'] = X_pca_cluster[:, i]\n",
    "\n",
    "df_out.to_csv(os.path.join(OUT_DIR, CLUSTERS_CSV), index=False)\n",
    "print(\"Saved clusters CSV to\", os.path.join(OUT_DIR, CLUSTERS_CSV))\n",
    "\n",
    "# ----------------------\n",
    "# Optional: show confusion / purity vs true labels\n",
    "# ----------------------\n",
    "def cluster_purity(true_labels, cluster_labels):\n",
    "    df = pd.DataFrame({'true': true_labels, 'cluster': cluster_labels})\n",
    "    total = len(df)\n",
    "    purity = 0.0\n",
    "    for c in df['cluster'].unique():\n",
    "        sub = df[df['cluster'] == c]\n",
    "        if len(sub) == 0: continue\n",
    "        top_count = sub['true'].value_counts().max()\n",
    "        purity += top_count\n",
    "    return purity / total\n",
    "\n",
    "purity_km = cluster_purity(true_labels, labels_kmeans)\n",
    "purity_agg = cluster_purity(true_labels, labels_agg)\n",
    "purity_db = cluster_purity(true_labels, labels_dbscan)\n",
    "\n",
    "pur_df = pd.DataFrame([{\n",
    "    'method': 'kmeans', 'purity': purity_km\n",
    "},{\n",
    "    'method': 'agg', 'purity': purity_agg\n",
    "},{\n",
    "    'method': 'dbscan', 'purity': purity_db\n",
    "}])\n",
    "pur_df.to_csv(os.path.join(OUT_DIR, 'cluster_purity.csv'), index=False)\n",
    "print(\"Saved cluster purity summary to\", os.path.join(OUT_DIR, 'cluster_purity.csv'))\n",
    "print(pur_df)\n",
    "\n",
    "print(\"All outputs saved to directory:\", os.path.abspath(OUT_DIR))\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eca250-edd7-43b2-821a-1c54e8347269",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
